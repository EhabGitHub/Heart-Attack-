{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8378649,"sourceType":"datasetVersion","datasetId":4982276}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/heart-attack'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-10T22:43:09.866566Z","iopub.execute_input":"2024-05-10T22:43:09.867440Z","iopub.status.idle":"2024-05-10T22:43:09.892588Z","shell.execute_reply.started":"2024-05-10T22:43:09.867402Z","shell.execute_reply":"2024-05-10T22:43:09.891583Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"/kaggle/input/heart-attack/heart_attack_prediction_dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Read the data into a DataFrame\ndata = pd.read_csv('/kaggle/input/heart-attack/heart_attack_prediction_dataset.csv')  # Replace 'your_file_path.csv' with the actual path to your CSV file\n\n# Print all columns\nprint(data.columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T22:43:12.935101Z","iopub.execute_input":"2024-05-10T22:43:12.935452Z","iopub.status.idle":"2024-05-10T22:43:12.977568Z","shell.execute_reply.started":"2024-05-10T22:43:12.935425Z","shell.execute_reply":"2024-05-10T22:43:12.976627Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Index(['Patient ID', 'Age', 'Sex', 'Cholesterol', 'Blood Pressure',\n       'Heart Rate', 'Diabetes', 'Family History', 'Smoking', 'Obesity',\n       'Alcohol Consumption', 'Exercise Hours Per Week', 'Diet',\n       'Previous Heart Problems', 'Medication Use', 'Stress Level',\n       'Sedentary Hours Per Day', 'Income', 'BMI', 'Triglycerides',\n       'Physical Activity Days Per Week', 'Sleep Hours Per Day', 'Country',\n       'Continent', 'Hemisphere', 'Heart Attack Risk'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef load_dataset():\n    data = pd.read_csv('/kaggle/input/heart-attack/heart_attack_prediction_dataset.csv')\n    # Preprocess 'Blood Pressure' column\n    data['Systolic Pressure'] = data['Blood Pressure'].apply(lambda x: int(x.split('/')[0]))\n    data['Diastolic Pressure'] = data['Blood Pressure'].apply(lambda x: int(x.split('/')[1]))\n    # Drop unnecessary columns\n    data = data.drop(columns=['Patient ID', 'Blood Pressure'])\n    # Encode categorical variables\n    data = pd.get_dummies(data, columns=['Sex'])\n    # Check if the column 'Heart Attack Risk' is present\n    if 'Heart Attack Risk' not in data.columns:\n        raise ValueError(\"Column 'Heart Attack Risk' not found in the dataset.\")\n    X = data.drop(columns=['Heart Attack Risk'])\n    y = data['Heart Attack Risk']\n    return X, y\n\ndef fitness_function(selected_features):\n    # Select only the features chosen by PSO\n    X_selected = X.iloc[:, selected_features]\n\n    # Split data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n    \n    # Train a Random Forest classifier\n    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n    clf.fit(X_train, y_train)\n    \n    # Evaluate accuracy on the test set\n    y_pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    return accuracy\n\n# Load the dataset\nX, y = load_dataset()\n\n# For demonstration, let's select some arbitrary features (e.g., features 0, 1, 2)\nselected_features = [0, 1, 2]\nresult = fitness_function(selected_features)\nprint(\"Accuracy:\", result)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T22:43:16.424836Z","iopub.execute_input":"2024-05-10T22:43:16.425817Z","iopub.status.idle":"2024-05-10T22:43:17.467941Z","shell.execute_reply.started":"2024-05-10T22:43:16.425783Z","shell.execute_reply":"2024-05-10T22:43:17.466861Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Accuracy: 0.5852823730747291\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\ndef initialize_swarm(num_particles, num_features):\n    \"\"\"\n    Initialize the particle swarm.\n\n    Parameters:\n    - num_particles (int): Number of particles in the swarm.\n    - num_features (int): Number of features in the dataset.\n\n    Returns:\n    - swarm (np.ndarray): Initial particle positions.\n    - velocities (np.ndarray): Initial particle velocities.\n    \"\"\"\n    # Initialize particle positions randomly\n    swarm = np.random.randint(2, size=(num_particles, num_features), dtype=bool)\n\n    # Initialize particle velocities randomly\n    velocities = np.random.rand(num_particles, num_features)\n\n    return swarm, velocities\n\n# Example usage:\nnum_particles = 10\nnum_features = X.shape[1]  # Number of features in the dataset\nswarm, velocities = initialize_swarm(num_particles, num_features)\nprint(\"Initial swarm shape:\", swarm.shape)\nprint(\"Initial velocities shape:\", velocities.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T22:43:20.142614Z","iopub.execute_input":"2024-05-10T22:43:20.142942Z","iopub.status.idle":"2024-05-10T22:43:20.150820Z","shell.execute_reply.started":"2024-05-10T22:43:20.142917Z","shell.execute_reply":"2024-05-10T22:43:20.149843Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Initial swarm shape: (10, 26)\nInitial velocities shape: (10, 26)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T22:43:24.177408Z","iopub.execute_input":"2024-05-10T22:43:24.177752Z","iopub.status.idle":"2024-05-10T22:43:24.182262Z","shell.execute_reply.started":"2024-05-10T22:43:24.177725Z","shell.execute_reply":"2024-05-10T22:43:24.181229Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef fitness_function(selected_features):\n    # Convert selected_features to a boolean array\n    selected_features = selected_features.astype(bool)\n    \n    # Find the indices of selected features\n    selected_indices = np.where(selected_features)[0]\n    \n    # Select column names based on selected_indices\n    selected_columns = X.columns[selected_indices]\n    \n    # Filter columns of X based on selected_columns\n    X_selected = X[selected_columns]\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n    \n    # Train a Random Forest classifier\n    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n    clf.fit(X_train, y_train)\n    \n    # Evaluate accuracy on the test set\n    accuracy = clf.score(X_test, y_test)\n    \n    return accuracy\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T22:43:26.581283Z","iopub.execute_input":"2024-05-10T22:43:26.582185Z","iopub.status.idle":"2024-05-10T22:43:26.589673Z","shell.execute_reply.started":"2024-05-10T22:43:26.582144Z","shell.execute_reply":"2024-05-10T22:43:26.588542Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming 'Northern Hemisphere' is a column in your DataFrame\n# Replace 'df' with the name of your DataFrame\ndf = pd.DataFrame({'Region': ['Northern Hemisphere', 'Southern Hemisphere', 'Equator']})\n\n# Perform one-hot encoding\ndf_encoded = pd.get_dummies(df, columns=['Region'])\n\n# Now 'Region' column is replaced with binary columns\nprint(df_encoded)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T22:43:31.934497Z","iopub.execute_input":"2024-05-10T22:43:31.934853Z","iopub.status.idle":"2024-05-10T22:43:31.944873Z","shell.execute_reply.started":"2024-05-10T22:43:31.934826Z","shell.execute_reply":"2024-05-10T22:43:31.943978Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"   Region_Equator  Region_Northern Hemisphere  Region_Southern Hemisphere\n0           False                        True                       False\n1           False                       False                        True\n2            True                       False                       False\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\n# Assuming 'data' is your DataFrame containing categorical variables\n# Replace 'data' with the name of your DataFrame\n# Perform one-hot encoding for categorical variables\ndata_encoded = pd.get_dummies(data)\n\n# Separate features (X) and target (y)\nX = data_encoded.drop(columns=['Heart Attack Risk'])\ny = data_encoded['Heart Attack Risk']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Random Forest classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Evaluate accuracy on the test set\naccuracy = clf.score(X_test, y_test)\nprint(\"Accuracy:\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T22:43:35.707362Z","iopub.execute_input":"2024-05-10T22:43:35.708153Z","iopub.status.idle":"2024-05-10T22:44:04.657555Z","shell.execute_reply.started":"2024-05-10T22:43:35.708124Z","shell.execute_reply":"2024-05-10T22:44:04.656587Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Accuracy: 0.6417569880205363\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\n# Assuming the number of particles is determined by the shape of swarm\nnum_particles = swarm.shape[0]\n\n# Initialize fitness_values as an array of zeros\nfitness_values = np.zeros(num_particles)\n\n# Evaluate fitness for each particle\nfor i in range(num_particles):\n    fitness_values[i] = fitness_function(swarm[i])\n\nprint(\"Fitness values for each particle:\", fitness_values)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T22:45:21.074213Z","iopub.execute_input":"2024-05-10T22:45:21.074548Z","iopub.status.idle":"2024-05-10T22:45:33.924935Z","shell.execute_reply.started":"2024-05-10T22:45:21.074523Z","shell.execute_reply":"2024-05-10T22:45:33.923999Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Fitness values for each particle: [0.61722761 0.60981175 0.60638905 0.61095265 0.62464347 0.61893896\n 0.62521392 0.62236167 0.61836851 0.61893896]\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\n# Function to update personal best positions and fitness values\ndef update_personal_best(particles, personal_best, fitness_values):\n    \"\"\"Update personal best positions and fitness values.\"\"\"\n    for i in range(len(particles)):\n        if fitness_values[i] > personal_best[i]['fitness']:\n            personal_best[i]['position'] = particles[i].copy()\n            personal_best[i]['fitness'] = fitness_values[i]\n\n# Function to update global best position and fitness value\ndef update_global_best(personal_best, global_best):\n    \"\"\"Update global best position and fitness value.\"\"\"\n    for pb in personal_best:\n        if pb['fitness'] > global_best['fitness']:\n            global_best['position'] = pb['position'].copy()\n            global_best['fitness'] = pb['fitness']\n\n# Example usage:\n# Assuming 'swarm' is your particles array\nswarm = np.array(swarm)\n\n# Initialize personal_best as a list of dictionaries\npersonal_best = [{'position': particle.copy(), 'fitness': -np.inf} for particle in swarm]\n\n# Example fitness values (replace this with your actual fitness values)\nfitness_values = np.random.rand(len(swarm))\n\n# Update personal best\nupdate_personal_best(swarm, personal_best, fitness_values)\n\n# Initialize global_best as a dictionary\nglobal_best = {'position': None, 'fitness': -np.inf}\n\n# Update global best\nupdate_global_best(personal_best, global_best)\n\nprint(\"Personal best positions and fitness values:\")\nfor i, pb in enumerate(personal_best):\n    print(f\"Particle {i+1}: Position = {pb['position']}, Fitness = {pb['fitness']}\")\n\nprint(f\"Global best position: {global_best['position']}, Global best fitness: {global_best['fitness']}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T22:45:46.872409Z","iopub.execute_input":"2024-05-10T22:45:46.873448Z","iopub.status.idle":"2024-05-10T22:45:46.886222Z","shell.execute_reply.started":"2024-05-10T22:45:46.873398Z","shell.execute_reply":"2024-05-10T22:45:46.885332Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Personal best positions and fitness values:\nParticle 1: Position = [False  True  True False False False  True  True False  True False False\n  True False False  True False False False False  True False  True  True\n  True False], Fitness = 0.6597295474015447\nParticle 2: Position = [False False False False False  True  True  True False  True False False\n  True  True False  True  True False False  True  True False  True False\n  True  True], Fitness = 0.034494318580179595\nParticle 3: Position = [False False False False  True False  True False  True False  True False\n  True False  True  True False False False False False False False  True\n  True  True], Fitness = 0.7512773477767123\nParticle 4: Position = [False False  True False  True  True  True False  True False  True False\n False False False  True  True False False False False  True False False\n False False], Fitness = 0.5118276621192402\nParticle 5: Position = [ True  True  True  True False False False False  True  True  True  True\n  True False False  True False  True False  True  True False  True  True\n False False], Fitness = 0.9688086805160089\nParticle 6: Position = [ True False  True False False False False False  True False  True  True\n False  True False False False False False False False  True False False\n False  True], Fitness = 0.5322178191181056\nParticle 7: Position = [False  True False False False False False False False  True  True False\n  True  True  True  True  True False  True False False  True False  True\n False False], Fitness = 0.32909021958831386\nParticle 8: Position = [False  True  True  True False False  True False  True  True False  True\n False False False False  True False False  True False  True  True False\n False False], Fitness = 0.5302072870197797\nParticle 9: Position = [ True False False False False False  True False False  True  True  True\n  True  True  True  True False False False  True  True False  True  True\n  True  True], Fitness = 0.8476025273821045\nParticle 10: Position = [False False False False False  True False  True  True  True  True False\n False False  True  True  True  True  True  True False  True False  True\n False  True], Fitness = 0.5929407949192332\nGlobal best position: [ True  True  True  True False False False False  True  True  True  True\n  True False False  True False  True False  True  True False  True  True\n False False], Global best fitness: 0.9688086805160089\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\ndef update_velocity(particles, velocities, personal_best, global_best, inertia_weight=0.5, cognitive_weight=1, social_weight=2):\n    updated_velocities = []\n    for i in range(len(particles)):\n        cognitive_component = cognitive_weight * np.random.rand() * (personal_best[i]['position'] - particles[i])\n        social_component = social_weight * np.random.rand() * (global_best['position'] - particles[i])\n        inertia_component = inertia_weight * velocities[i]\n        updated_velocity = inertia_component + cognitive_component + social_component\n        updated_velocities.append(updated_velocity)\n    return updated_velocities\n\ndef update_position(particles, velocities):\n    updated_particles = particles + velocities\n    return updated_particles\n\n# Example usage:\nparticles = np.random.rand(10, 26)  # Example particle positions\nvelocities = np.random.rand(10, 26)  # Example velocities\npersonal_best = [{'position': np.random.rand(26), 'fitness': np.random.rand()} for _ in range(10)]  # Example personal bests\nglobal_best = {'position': np.random.rand(26), 'fitness': np.random.rand()}  # Example global best\n\nupdated_velocities = update_velocity(particles, velocities, personal_best, global_best)\nupdated_particles = update_position(particles, updated_velocities)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T22:45:51.140246Z","iopub.execute_input":"2024-05-10T22:45:51.140949Z","iopub.status.idle":"2024-05-10T22:45:51.152369Z","shell.execute_reply.started":"2024-05-10T22:45:51.140916Z","shell.execute_reply":"2024-05-10T22:45:51.151189Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Example fitness function\ndef fitness_function(position):\n    # Example fitness function (can be replaced with any other fitness function)\n    return np.sum(position)\n\ndef update_velocity_and_position(particles, velocities, personal_best, global_best, w, c1, c2):\n    \"\"\"\n    Update particle velocities and positions based on PSO equations.\n    \"\"\"\n    for i in range(len(particles)):\n        r1 = np.random.random(size=len(particles[i]))\n        r2 = np.random.random(size=len(particles[i]))\n        velocities[i] = w * velocities[i] + c1 * r1 * (personal_best[i] - particles[i]) + c2 * r2 * (global_best - particles[i])\n        particles[i] = particles[i] + velocities[i]\n\ndef update_personal_best(particles, personal_best, fitness_values):\n    \"\"\"\n    Update personal best positions and fitness values.\n    \"\"\"\n    for i in range(len(particles)):\n        if fitness_values[i] > personal_best[i]['fitness']:\n            personal_best[i]['position'] = particles[i].copy()\n            personal_best[i]['fitness'] = fitness_values[i]\n    return personal_best\n\ndef update_global_best(personal_best, global_best):\n    \"\"\"\n    Update global best position and fitness.\n    \"\"\"\n    for i in range(len(personal_best)):\n        if personal_best[i]['fitness'] > global_best['fitness']:\n            global_best['position'] = personal_best[i]['position'].copy()\n            global_best['fitness'] = personal_best[i]['fitness']\n    return global_best\n\ndef pso_main_loop(num_particles, num_dimensions, num_iterations, w, c1, c2):\n    \"\"\"\n    Perform the main loop of the Particle Swarm Optimization (PSO) algorithm.\n    \"\"\"\n    # Initialize particles and velocities\n    particles = np.random.rand(num_particles, num_dimensions)\n    velocities = np.random.rand(num_particles, num_dimensions)\n    \n    # Initialize personal best\n    personal_best = [{'position': particles[i].copy(), 'fitness': -np.inf} for i in range(num_particles)]\n    \n    # Initialize global best\n    global_best = {'position': np.zeros(num_dimensions), 'fitness': -np.inf}\n    \n    # Main loop\n    for _ in range(num_iterations):\n        # Update velocity and position\n        update_velocity_and_position(particles, velocities, [x['position'] for x in personal_best], global_best['position'], w, c1, c2)\n        \n        # Evaluate fitness for each particle\n        fitness_values = np.array([fitness_function(p) for p in particles])\n        \n        # Update personal best\n        personal_best = update_personal_best(particles, personal_best, fitness_values)\n        \n        # Update global best\n        global_best = update_global_best(personal_best, global_best)\n    \n    return global_best\n\n# Example usage\nnum_particles = 10\nnum_dimensions = 5\nnum_iterations = 50\nw = 0.5\nc1 = 1.5\nc2 = 2.0\n\nfinal_global_best = pso_main_loop(num_particles, num_dimensions, num_iterations, w, c1, c2)\n\nprint(\"Final global best position:\", final_global_best['position'])\nprint(\"Final global best fitness:\", final_global_best['fitness'])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T22:46:00.511160Z","iopub.execute_input":"2024-05-10T22:46:00.512060Z","iopub.status.idle":"2024-05-10T22:46:00.544111Z","shell.execute_reply.started":"2024-05-10T22:46:00.512015Z","shell.execute_reply":"2024-05-10T22:46:00.543212Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Final global best position: [ 21.65301615 483.47856395   4.01378115  14.59136338   8.81068243]\nFinal global best fitness: 532.5474070574286\n","output_type":"stream"}]},{"cell_type":"code","source":"# Display the columns in the dataset\nprint(\"Columns in the dataset:\")\nprint(data.columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T23:33:29.619727Z","iopub.execute_input":"2024-05-10T23:33:29.620574Z","iopub.status.idle":"2024-05-10T23:33:29.625420Z","shell.execute_reply.started":"2024-05-10T23:33:29.620541Z","shell.execute_reply":"2024-05-10T23:33:29.624484Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Columns in the dataset:\nIndex(['Patient ID', 'Age', 'Sex', 'Cholesterol', 'Blood Pressure',\n       'Heart Rate', 'Diabetes', 'Family History', 'Smoking', 'Obesity',\n       'Alcohol Consumption', 'Exercise Hours Per Week', 'Diet',\n       'Previous Heart Problems', 'Medication Use', 'Stress Level',\n       'Sedentary Hours Per Day', 'Income', 'BMI', 'Triglycerides',\n       'Physical Activity Days Per Week', 'Sleep Hours Per Day', 'Country',\n       'Continent', 'Hemisphere', 'Heart Attack Risk'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Preprocess the 'Blood Pressure' column\ndata['Blood Pressure'] = data['Blood Pressure'].apply(lambda x: sum(map(int, x.split('/'))) / 2)\n\n# Define selected features\nselected_features = ['Age', 'Sex', 'Cholesterol', 'Blood Pressure', 'Heart Rate']\n\n# Separate features and target variable\nX_original = data[selected_features]\ny_original = data['Heart Attack Risk']\n\n# One-hot encode the 'Sex' column\nX_original = pd.get_dummies(X_original, columns=['Sex'], drop_first=True)\n\n# Split the original dataset into training and testing sets\nX_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_original, y_original, test_size=0.2, random_state=42)\n\n# Train a machine learning model (Random Forest) on the original dataset\nrf_orig = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_orig.fit(X_train_orig, y_train_orig)\n\n# Predict on the testing set using the model trained on the original dataset\ny_pred_orig = rf_orig.predict(X_test_orig)\n\n# Calculate performance metrics for the model trained on the original dataset\naccuracy_orig = accuracy_score(y_test_orig, y_pred_orig)\nprecision_orig = precision_score(y_test_orig, y_pred_orig, average='weighted')\nrecall_orig = recall_score(y_test_orig, y_pred_orig, average='weighted')\nf1_orig = f1_score(y_test_orig, y_pred_orig, average='weighted')\n\n# Display performance metrics for the original dataset\nprint(\"Performance metrics for the original dataset:\")\nprint(\"Accuracy:\", accuracy_orig)\nprint(\"Precision:\", precision_orig)\nprint(\"Recall:\", recall_orig)\nprint(\"F1-score:\", f1_orig)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T23:35:44.352757Z","iopub.execute_input":"2024-05-10T23:35:44.353604Z","iopub.status.idle":"2024-05-10T23:35:45.709973Z","shell.execute_reply.started":"2024-05-10T23:35:44.353565Z","shell.execute_reply":"2024-05-10T23:35:45.708997Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Performance metrics for the original dataset:\nAccuracy: 0.6012549914432401\nPrecision: 0.5370749872163881\nRecall: 0.6012549914432401\nF1-score: 0.541190183166748\n","output_type":"stream"}]}]}